# Image Classification

## Model Training Speed Comparison

### CPU vs. GPU

| **Task** | **CPU** | **GPU (64)** |
|-----|-----|-----|
| Learning Loop | 12m 37s | 1m 41s |

### GPU: Batch Sizes

| **Task** | **16** | **32** | **64** | **128** | **256** |
|-----|-----|-----|-----|-----|-----|
| Learning Loop | 2m 31s | 2m 5s | 1m 41s | 1m 28s | 1m 44s |

- Batch size is the number of training examples a model processes before updating its internal parameters
- The model averages the error across a batch of images before update instead of updating for every single image
- Batch size affect: model training speed, memory usage and time per step
   - Larger batch size gives fewer steps and train model faster
   - Small batch size uses less memory
   - Large batch size has more stable gradients that are also smoother
   - Small batch size has more noise in gradient updates
   - Large batch size may overfit if the hardware is not powerful enough
- Default batch size is 64
   - Batch size can be updated with `bs=` when creating dataloaders (dls)

## Classification Loss Functions

Measures the accuracy for predictions of a model, less the number means higher accuracy

### Binary Classification

1. Binary Cross Entropy Loss
   - Used for binary and multi-class classification problems
   - Predicts the probability for a single positive class
   - Compares the predicted probability with the true binary label for each example
     
2. Hinge Loss
   - Used for binary classification problems
   - Focuse on correctly classifying data
   - Apply penalty when the prediction is on the wrong side of the margin

### Multi-Class Classification [Single Label]

1. Mean Squared Error Loss
   - Used when there are continuous value
   - Measure average square difference between the predicted values and the true labels
   - Sensitive to outliers
   
2. Mean Bias Error Loss
   - Used in regression tasks to measure the average bias in the predictions
   - Focus on the direction of the error
   - Bias is positive when over-predict, negative when under-predict

### Multi-Class Classification [Multi-Label]

1. Multi-Class Cross Entropy Loss
   - Used when there are more than two classes.
   - Each example belongs to exactly one class.
   - Compares the predicted probability distribution with the true class label, penalizing incorrect or low-confidence predictions.
     
2. Binary Cross Entropy Loss

## Data Analysation

### Confusion matrix

- A table that shows how well the model's predictions match the true labels
- Each column represents a predicted class, and each row represents a true class
- Diagonal cells from top-left to bottom-right show correct predictions, and off-diagonal cells show misclassifications
- Displays which classes the model has confused

### t-SNE

- t-Distributed Stochastic Neighbor Embedding
- A technique for visualizing how well the model separates different classes
- Used to measure the similarity of data points in the original space
- Generates a graph with similar points close together and dissimilar points farther apart

## Classify Images

Model: `ResNet18` (a type of convolutional neural network designed for image classification tasks)

Loss Function: Multi-class cross entropy loss

### Aims

- Develop an image classification system based on `00-is-it-a-bird-creating-a-model-from-your-own-data.ipynb` from [lovellbrian/course22](https://github.com/lovellbrian/course22)
- Target classes for classification: airplane, automobile, bird, cat, and dog
- Use DuckDuckGo to scrape sample images
- Design or select an appropriate multiclass loss function

### Results

Downloaded Images

![](/images/study/elec4630-course22/classification_1.png)

Model Training

![](/images/study/elec4630-course22/classification_2.png)

Confusion matrix

![](/images/study/elec4630-course22/classification_3.png)

t-SNE

![](/images/study/elec4630-course22/classification_4.png)

Example

![](/images/study/elec4630-course22/classification_5.png)

### Feedback

- GPU learning environment is based on Nvidia GPUs, which causes errors on devices with Apple silicon and Intel chips
- Format of downloaded images may cause errors
- Learning loop takes longer than expected when processing an increased number of images
- Although the accuracy shows above 0.9, there are still obvious classification errors (according to t-SNE)
